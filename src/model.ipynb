{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Askllama-reddit: Fine-tuning Llama-2-7b on Reddit ML Discussions\n",
    "\n",
    "This notebook fine-tunes Meta's Llama-2-7b model on Reddit machine learning discussion data\n",
    "using QLoRA (4-bit quantization + LoRA adapters) for parameter-efficient training.\n",
    "\n",
    "**Requirements:** Google Colab with a T4 GPU (free tier works).\n",
    "\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Prepare data (deduplicate, format, split)\n",
    "3. Load base model with 4-bit quantization\n",
    "4. Configure LoRA adapters\n",
    "5. Train with SFTTrainer\n",
    "6. Evaluate and visualize training loss\n",
    "7. Merge adapters and save final model\n",
    "8. Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 0: Install dependencies\n!pip install -q \\\n    transformers>=4.36.0 \\\n    trl>=0.7.0 \\\n    peft>=0.7.0 \\\n    accelerate>=0.25.0 \\\n    datasets>=2.14.0 \\\n    bitsandbytes>=0.41.0 \\\n    huggingface_hub \\\n    sentencepiece \\\n    protobuf \\\n    einops \\\n    scipy \\\n    matplotlib\n\n# Optional: install wandb for experiment tracking (skip if you don't have an account)\n# !pip install -q wandb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Hugging Face login\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login to Hugging Face (needed for Llama-2 gated model access)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Upload data and prepare it\n",
    "# Upload your custjsonl.jsonl file to Colab, or mount Google Drive.\n",
    "#\n",
    "# Option A: Upload directly\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # upload custjsonl.jsonl\n",
    "#\n",
    "# Option B: Mount Google Drive (if data is stored there)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "RAW_DATA_PATH = \"custjsonl.jsonl\"  # adjust path if using Drive\n",
    "\n",
    "# --- Data preparation (inline version of scripts/prepare_data.py) ---\n",
    "MIN_COMMENT_LENGTH = 10\n",
    "VAL_RATIO = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def format_prompt(title, post_content, comments):\n",
    "    return (\n",
    "        f\"### Post Title:\\n{title.strip()}\\n\\n\"\n",
    "        f\"### Post Content:\\n{post_content.strip()}\\n\\n\"\n",
    "        f\"### Top Comments:\\n{comments.strip()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Load\n",
    "raw_records = []\n",
    "with open(RAW_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                raw_records.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "print(f\"Raw records: {len(raw_records)}\")\n",
    "\n",
    "# Deduplicate\n",
    "seen = set()\n",
    "unique = []\n",
    "for r in raw_records:\n",
    "    key = (r.get(\"title\", \"\"), r.get(\"post_content\", \"\"), r.get(\"comments\", \"\"))\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        unique.append(r)\n",
    "\n",
    "print(f\"After deduplication: {len(unique)}\")\n",
    "\n",
    "# Filter short comments\n",
    "filtered = [r for r in unique if len(r.get(\"comments\", \"\").strip()) >= MIN_COMMENT_LENGTH]\n",
    "print(f\"After filtering: {len(filtered)}\")\n",
    "\n",
    "# Format prompts\n",
    "for r in filtered:\n",
    "    r[\"text\"] = format_prompt(r[\"title\"], r[\"post_content\"], r[\"comments\"])\n",
    "\n",
    "# Split\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(filtered)\n",
    "val_size = max(1, int(len(filtered) * VAL_RATIO))\n",
    "val_data = filtered[:val_size]\n",
    "train_data = filtered[val_size:]\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "\n",
    "# Convert to HF Datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "# Preview\n",
    "print(\"\\n--- Sample formatted entry ---\")\n",
    "print(train_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load base model with 4-bit quantization\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model loaded. GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Configure LoRA and training\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\n\nOUTPUT_DIR = \"./results\"\n\n# Set report_to=\"wandb\" if you have a wandb account, otherwise use \"none\"\nREPORT_TO = \"none\"  # Change to \"wandb\" to enable experiment tracking\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",\n    save_steps=100,\n    warmup_steps=30,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=REPORT_TO,\n)\n\nMAX_SEQ_LENGTH = 512\n\ntrainer = SFTTrainer(\n    model=base_model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    tokenizer=tokenizer,\n    args=training_args,\n)\n\nprint(f\"Trainer initialized.\")\nprint(f\"  Trainable params: {sum(p.numel() for p in base_model.parameters() if p.requires_grad):,}\")\nprint(f\"  Total params: {sum(p.numel() for p in base_model.parameters()):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train\n",
    "torch.cuda.empty_cache()\n",
    "train_result = trainer.train()\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Total steps: {trainer.state.global_step}\")\n",
    "print(f\"  Final train loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize training loss\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_steps = [entry[\"step\"] for entry in log_history if \"loss\" in entry]\n",
    "train_losses = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
    "\n",
    "eval_steps = [entry[\"step\"] for entry in log_history if \"eval_loss\" in entry]\n",
    "eval_losses = [entry[\"eval_loss\"] for entry in log_history if \"eval_loss\" in entry]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_steps, train_losses, label=\"Training Loss\", alpha=0.8)\n",
    "if eval_losses:\n",
    "    plt.plot(eval_steps, eval_losses, label=\"Validation Loss\", marker=\"o\", alpha=0.8)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Save LoRA adapters and merge into base model\nimport os\nimport gc\n\n# Save the LoRA adapters\nadapter_dir = os.path.join(OUTPUT_DIR, \"final_adapter\")\ntrainer.model.save_pretrained(adapter_dir)\ntokenizer.save_pretrained(adapter_dir)\nprint(f\"LoRA adapters saved to {adapter_dir}\")\n\n# Free GPU memory from training before merging\n# (T4 only has 16GB â€” can't hold both the training model and merge model)\ndel trainer\ndel base_model\ngc.collect()\ntorch.cuda.empty_cache()\nprint(f\"\\nFreed GPU memory. Current usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n\n# Merge LoRA into base model for easier inference\nprint(\"Merging LoRA adapters into base model...\")\nmerged_dir = os.path.join(OUTPUT_DIR, \"merged\")\n\n# Reload base model in fp16 for merging\nbase_model_for_merge = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Load and merge adapters\nmerged_model = PeftModel.from_pretrained(base_model_for_merge, adapter_dir)\nmerged_model = merged_model.merge_and_unload()\n\n# Save merged model\nmerged_model.save_pretrained(merged_dir)\ntokenizer.save_pretrained(merged_dir)\nprint(f\"Merged model saved to {merged_dir}\")\n\n# Clean up\ndel merged_model, base_model_for_merge\ngc.collect()\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the merged model for inference\n",
    "merged_dir = os.path.join(OUTPUT_DIR, \"merged\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_dir,\n",
    "    tokenizer=merged_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"### Post Title:\\nWhat is the best way to fine-tune a large language model?\\n\\n### Post Content:\\nI have a dataset of domain-specific text and want to adapt an LLM. What approaches work best for a 7B model?\\n\\n### Top Comments:\",\n",
    "    \"### Post Title:\\nHow does LoRA compare to full fine-tuning?\\n\\n### Post Content:\\nI'm considering using LoRA for my project. What are the trade-offs vs full fine-tuning?\\n\\n### Top Comments:\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}